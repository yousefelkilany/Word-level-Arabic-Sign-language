{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5612889,"sourceType":"datasetVersion","datasetId":3211537}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\ngdrive_kaggle_wl_ar_sl = user_secrets.get_secret(\"gdrive_kaggle_wl_ar_sl\")\n\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\ndef upload_to_gdrive(file_name, file_content):\n    file_metadata = {\n        'title': file_name,\n        'parents': [{'id': gdrive_kaggle_wl_ar_sl}]\n    }\n    file = drive.CreateFile(file_metadata)\n    file.SetContentString(file_content)\n    file.Upload() # Files.insert()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T19:30:16.546713Z","iopub.execute_input":"2025-08-04T19:30:16.547126Z","iopub.status.idle":"2025-08-04T19:31:32.946005Z","shell.execute_reply.started":"2025-08-04T19:30:16.547092Z","shell.execute_reply":"2025-08-04T19:31:32.945229Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!export TF_CPP_MIN_LOG_LEVEL=2\n!pip install -q opencv-python mediapipe matplotlib\n!wget -O KARSL-502_Labels.xlsx -q https://github.com/issamjebnouni/Arabic-Word-level-Sign-Language-Recognition/raw/refs/heads/main/KARSL-502_Labels.xlsx\n!wget -O hand_landmarker.task -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\n!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-05T04:05:58.909519Z","iopub.execute_input":"2025-08-05T04:05:58.909909Z","iopub.status.idle":"2025-08-05T04:06:07.722741Z","shell.execute_reply.started":"2025-08-05T04:05:58.909879Z","shell.execute_reply":"2025-08-05T04:06:07.721321Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def upload_to_gdrive(file_name, file_content):\n    file_metadata = {\n        'title': file_name,\n        'parents': [{'id': gdrive_kaggle_wl_ar_sl}]\n    }\n    file = drive.CreateFile(file_metadata)\n    file.SetContentString(file_content)\n    file.Upload() # Files.insert()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport mediapipe as mp\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\nfrom concurrent.futures import ThreadPoolExecutor\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # '2' suppresses warnings and info messages\nos_join = os.path.join\n\nDATA_DIR = \"/kaggle/input/karsl-502\"\nKPS_DIR = \"/kaggle/working/karsl-kps\"\n\nmp_pose_options = vision.PoseLandmarkerOptions(\n    base_options=BaseOptions(model_asset_path='pose_landmarker.task'),\n    running_mode=VisionRunningMode.VIDEO\n)\nmp_face_options = vision.FaceLandmarkerOptions(\n    base_options=BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task'),\n    running_mode=VisionRunningMode.VIDEO,\n    num_faces=1\n)\nmp_hands_options = vision.HandLandmarkerOptions(\n    base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n    running_mode=VisionRunningMode.VIDEO,\n    num_hands=2\n)\n\nmp_pose_nose_idx = mp.solutions.pose.PoseLandmark.NOSE\nmp_face_nose_idx = sorted(mp.solutions.face_mesh_connections.FACEMESH_NOSE)[0][0]\nmp_hand_wrist_idx = mp.solutions.hands.HandLandmark.WRIST\n\npose_kps_idx = tuple(\n    (\n        mp.solutions.pose.PoseLandmark.LEFT_SHOULDER,\n        mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER,\n        mp.solutions.pose.PoseLandmark.LEFT_ELBOW,\n        mp.solutions.pose.PoseLandmark.RIGHT_ELBOW,\n        mp.solutions.pose.PoseLandmark.LEFT_WRIST,\n        mp.solutions.pose.PoseLandmark.RIGHT_WRIST,\n    )\n)\nface_kps_idx = tuple(\n    sorted(\n        set(\n            point\n            for edge in [\n                *mp.solutions.face_mesh_connections.FACEMESH_CONTOURS,\n                *mp.solutions.face_mesh_connections.FACEMESH_IRISES,\n            ]\n            for point in edge\n        )\n    )\n)\nhand_kps_idx = tuple(range(len(mp.solutions.hands.HandLandmark)))\n\nPOSE_NUM = len(pose_kps_idx)\nFACE_NUM = len(face_kps_idx)\nHAND_NUM = len(hand_kps_idx)\n\nKP2SLICE = {\n    \"pose\": slice(0, POSE_NUM),\n    \"face\": slice(POSE_NUM, POSE_NUM + FACE_NUM),\n    \"rh\": slice(POSE_NUM + FACE_NUM, POSE_NUM + FACE_NUM + HAND_NUM),\n    \"lh\": slice(POSE_NUM + FACE_NUM + HAND_NUM, POSE_NUM + FACE_NUM + HAND_NUM * 2),\n}\nPOSE_KPS2IDX = {kps: idx for idx, kps in enumerate(pose_kps_idx)}\nFACE_KPS2IDX = {kps: idx for idx, kps in enumerate(face_kps_idx)}\nHAND_KPS2IDX = {kps: idx for idx, kps in enumerate(hand_kps_idx)}\nKPS2IDX = {\"pose\": POSE_KPS2IDX, \"face\": FACE_KPS2IDX, \"hand\": HAND_KPS2IDX}\n\n\n# usage: use it to draw mediapipe connections with the kps loaded from `.npy`arrays\nfor u, v in list(mp.solutions.face_mesh_connections.FACEMESH_IRISES)[:3]:\n    print(face_kps_idx[FACE_KPS2IDX[u]], face_kps_idx[FACE_KPS2IDX[v]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T04:57:37.748720Z","iopub.execute_input":"2025-08-05T04:57:37.749039Z","iopub.status.idle":"2025-08-05T04:57:37.762722Z","shell.execute_reply.started":"2025-08-05T04:57:37.749018Z","shell.execute_reply":"2025-08-05T04:57:37.761372Z"}},"outputs":[{"name":"stdout","text":"475 476\n477 474\n469 470\n","output_type":"stream"}],"execution_count":145},{"cell_type":"code","source":"def get_karsl_words_min_frames_cnt():\n    in_dir = \"/kaggle/input/karsl-502\"\n    words_frames = defaultdict(lambda: (0, None))\n    for signer in tqdm([\"01\", \"02\", \"03\"], desc=\"signer\"):\n        signer_dir = os_join(in_dir, signer, signer)\n\n        for split in tqdm([\"train\", \"test\"], desc=\"split\", leave=False):\n            split_dir = os_join(signer_dir, split)\n\n            for word in tqdm(range(1, 503), desc=\"words\", leave=False):\n                frames = (999, None)\n                word_dir = os_join(split_dir, f\"{word:04}\")\n\n                for rep in os.listdir(word_dir):\n                    frames_dir = os_join(word_dir, rep)\n                    frames_cnt = len(os.listdir(frames_dir))\n                    if frames_cnt < frames[0]:\n                        frames = (frames_cnt, frames_dir)\n\n                if frames[0] > words_frames[word][0]:\n                    words_frames[word] = frames\n    return words_frames\n\n\n# words_frames = get_karsl_words_min_frames_cnt()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bad_samples = [\n    # this sample has >260 frames, and after inspection it has many unrelated frames, so just drop it\n    'karsl-502/02/02/train/0443/03_02_0443_(15_11_17_15_52_07)_c',\n]\n\nMS_30FPS = 1000/30\nPAD_TKN = -1\nSEQ_LEN = 80","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T04:50:20.206102Z","iopub.execute_input":"2025-08-05T04:50:20.206431Z","iopub.status.idle":"2025-08-05T04:50:20.212424Z","shell.execute_reply.started":"2025-08-05T04:50:20.206406Z","shell.execute_reply":"2025-08-05T04:50:20.211520Z"}},"outputs":[],"execution_count":139},{"cell_type":"code","source":"image_dir = '/kaggle/input/karsl-502/01/01/train/0001/01_01_0001_(10_11_16_16_21_34)_c/'\nimage_path = os.path.join(image_dir, os.listdir(image_dir)[8])\n# shutil.copy(image_path, '/kaggle/working')\nframe = mp.Image.create_from_file(image_path)\n# mp_image = Image(image_format=ImageFormat.SRGB, data=image)\n\n\nimport mediapipe as mp\nfrom mediapipe.tasks.python import BaseOptions, vision\nVisionRunningMode = vision.RunningMode\n\n# pose_model = vision.PoseLandmarker.create_from_options(pose_options)\n# pose_res = pose_model.detect(frame)\n# print(dir(pose_res))\n# print(len(pose_res.pose_landmarks[0]))\n\n# face_model = vision.FaceLandmarker.create_from_options(face_options)\n# face_res = face_model.detect(frame)\n# print(dir(face_res))\n# print(face_res.facial_transformation_matrixes)\n# print(len(face_res.face_landmarks[0]))\n\nhands_model = vision.HandLandmarker.create_from_options(hands_options)\nhands_res = hands_model.detect_for_video(frame, 0)\nprint(dir(hands_res))\nprint(hands_res.handedness[0])\nprint(len(hands_res.hand_landmarks[0]))\n\n# annotated_image = draw_landmarks_on_image(frame.numpy_view(), hands_res)\n# annotated_image = draw_pose_andmarks_on_image(frame.numpy_view(), pose_res)\n# cv2.imwrite(\"drawn.jpg\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:20:30.812113Z","iopub.execute_input":"2025-08-05T05:20:30.812495Z","iopub.status.idle":"2025-08-05T05:20:30.927345Z","shell.execute_reply.started":"2025-08-05T05:20:30.812473Z","shell.execute_reply":"2025-08-05T05:20:30.926395Z"}},"outputs":[{"name":"stdout","text":"['__annotations__', '__class__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'hand_landmarks', 'hand_world_landmarks', 'handedness']\n[Category(index=1, score=0.9807953238487244, display_name='Left', category_name='Left')]\n21\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754371230.826115      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371230.868143     819 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371230.884169     819 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"}],"execution_count":155},{"cell_type":"code","source":"def extract_frame_keypoints(frame_path, pose_model, face_model, hands_model, timestamp):\n    # TODO: normalize(?) keypoints after adjustment\n\n    def get_xyz(lm):\n        return (lm.x, lm.y, lm.z)\n\n    # define numpy views, pose -> face -> rh -> lh\n    all_kps = np.zeros((184, 3))  # (pose=6 + face=136 + rh+lh=42), xyz=3\n    pose_kps = all_kps[KP2SLICE[\"pose\"]]\n    face_kps = all_kps[KP2SLICE[\"face\"]]\n    rh_kps = all_kps[KP2SLICE[\"rh\"]]\n    lh_kps = all_kps[KP2SLICE[\"lh\"]]\n    np_xyz = np.dtype((float, 3))\n\n    frame = mp.Image.create_from_file(frame_path)\n    \n    def get_pose():\n        nonlocal pose_kps\n        results = pose_model.detect_for_video(frame, timestamp)\n        if results.pose_landmarks is None:\n            return\n\n        lms = results.pose_landmarks[0]\n        pose_kps[:] = np.fromiter(((lms[idx].x, lms[idx].y, lms[idx].z) for idx in pose_kps_idx), dtype=np_xyz)\n        # pose_kps -= pose_kps[mp_pose_nose_idx]\n\n    def get_face():\n        nonlocal face_kps\n        results = face_model.detect_for_video(frame, timestamp)\n        if results.face_landmarks is None:\n            return\n\n        lms = results.face_landmarks[0]\n        face_kps[:] = np.fromiter(((lms[idx].x, lms[idx].y, lms[idx].z) for idx in face_kps_idx), dtype=np_xyz)\n        # face_kps -= face_kps[mp_face_nose_idx]\n\n    def get_hands():\n        nonlocal rh_kps, lh_kps\n        results = hands_model.detect_for_video(frame, timestamp)\n        if results.hand_landmarks is None:\n            return\n\n        for handedness, hand_lms in zip(hands_res.handedness, hands_res.hand_landmarks):\n            target_hand = lh_kps if handedness[0].category_name == 'Left' else rh_kps\n            # print(handedness[0].category_name)\n            # print(hand_lms)\n            target_hand[:] = np.fromiter(((lm.x, lm.y, lm.z) for lm in hand_lms), dtype=np_xyz)\n\n\n    get_pose()\n    get_face()\n    get_hands()\n    # with ThreadPoolExecutor(max_workers=3) as executor:\n    #     executor.submit(get_pose)\n    #     executor.submit(get_face)\n    #     executor.submit(get_hands)\n    \n    return all_kps\n\n\ndef store_keypoint_arrays(models, word_dir, out_dir, split, signer, word, max_videos):\n    [pose_model, face_model, hands_model] = models\n\n    all_kps = []\n    videos_bar = tqdm(os.listdir(word_dir)[:max_videos], leave=False)\n    for video in videos_bar:\n        video_dir = os_join(word_dir, video)\n        videos_bar.set_description(f\"Current video: {video}\")\n\n        video_kps = []\n        hands_model = vision.HandLandmarker.create_from_options(mp_hands_options)\n        face_model = vision.FaceLandmarker.create_from_options(mp_face_options)\n        pose_model = vision.PoseLandmarker.create_from_options(mp_pose_options)\n        with pose_model, face_model, hands_model:\n            for idx, frame in enumerate(sorted(os.listdir(video_dir))):\n                frame_path = os_join(video_dir, frame)\n                timestamp = int(idx * MS_30FPS)\n                video_kps.append(\n                    extract_frame_keypoints(frame_path, pose_model, face_model, hands_model, timestamp)\n                )\n    \n            all_kps.append(video_kps.copy())\n            print(np.concatenate(all_kps, axis=0).shape)\n\n    # word_kps_path = os_join(out_dir, \"all_kps\", f\"{signer}-{split}\", word)\n    # os.makedirs(os.dirname(word_kps_path), exist_ok=True)\n    # np.savez(word_kps_path, keypoints=np.concatenate(all_kps, axis=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:28:12.071289Z","iopub.execute_input":"2025-08-05T05:28:12.071605Z","iopub.status.idle":"2025-08-05T05:28:12.088706Z","shell.execute_reply.started":"2025-08-05T05:28:12.071584Z","shell.execute_reply":"2025-08-05T05:28:12.087137Z"}},"outputs":[],"execution_count":169},{"cell_type":"code","source":"def extract_keypoints_from_frames(data_dir, kps_dir, splits=None, signers=None, selected_words=None):\n    pose_model = mp_pose.Pose()\n    face_model = mp_face.FaceMesh(refine_landmarks=True)\n    hands_model = mp_hands.Hands()\n    models = [pose_model, face_model, hands_model]\n\n    splits = splits or [\"train\", \"test\"]\n    signers = signers or [\"01\", \"02\", \"03\"]\n    selected_words = selected_words or tuple((f\"{v:04}\" for v in range(46, 503)))\n    words_bar = tqdm(selected_words)\n    for word in words_bar:\n        words_bar.set_description(f\"Current word: {word}\")\n        signers_bar = tqdm(signers, leave=False)\n        for signer in signers:\n            signers_bar.set_description(f\"Current signer: {signer}\")\n            splits_bar = tqdm(splits, leave=False)\n            for split in splits:\n                splits_bar.set_description(f\"Current split: {split}\")\n                word_dir = os_join(data_dir, signer, signer, split, word)\n                store_keypoint_arrays(models, word_dir, kps_dir, split, signer, word, max_videos=10)\n    \nextract_keypoints_from_frames(DATA_DIR, KPS_DIR)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-05T05:28:13.380395Z","iopub.execute_input":"2025-08-05T05:28:13.381821Z","iopub.status.idle":"2025-08-05T05:28:28.748265Z","shell.execute_reply.started":"2025-08-05T05:28:13.381783Z","shell.execute_reply":"2025-08-05T05:28:28.746913Z"}},"outputs":[{"name":"stderr","text":"W0000 00:00:1754371693.425382     974 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371693.450278     973 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/457 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40f0ac60db174f32a95f7bba12a6a720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a86551702ab410181a41e206aef64ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcba17c9725b40bda1208bc313d08570"}},"metadata":{}},{"name":"stderr","text":"W0000 00:00:1754371693.501042     977 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce8ec7076b8d470ebc8aaded9cae7842"}},"metadata":{}},{"name":"stderr","text":"W0000 00:00:1754371693.519594     970 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371693.523784      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371693.552563     977 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371693.598519     981 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371693.605910     970 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371693.648027     983 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371693.649025      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371693.649386      36 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\nW0000 00:00:1754371693.656489     985 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371693.673259     987 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371693.676880      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371693.770280     989 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371694.066592     992 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"},{"name":"stdout","text":"(19, 184, 3)\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754371697.356472      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371697.376730     993 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371697.389946     993 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371697.398476      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371697.398905      36 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\nW0000 00:00:1754371697.406085     997 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371697.435825     997 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371697.440959      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371697.511774    1002 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371697.650927    1003 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"},{"name":"stdout","text":"(45, 184, 3)\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754371701.719348      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371701.769074    1006 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371701.801472    1006 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371701.804252      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371701.804598      36 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\nW0000 00:00:1754371701.814278    1011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371701.848328    1011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371701.856441      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371701.966799    1014 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371702.158893    1014 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"},{"name":"stdout","text":"(66, 184, 3)\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754371705.553206      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371705.600524    1018 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371705.633355    1018 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371705.635939      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371705.636277      36 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\nW0000 00:00:1754371705.646405    1023 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371705.681660    1023 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nI0000 00:00:1754371705.691224      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754371705.784154    1028 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754371705.995802    1028 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4138623933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mstore_keypoint_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkps_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_videos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mextract_keypoints_from_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKPS_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/4138623933.py\u001b[0m in \u001b[0;36mextract_keypoints_from_frames\u001b[0;34m(data_dir, kps_dir, splits, signers, selected_words)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0msplits_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Current split: {split}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mword_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mstore_keypoint_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkps_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_videos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mextract_keypoints_from_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKPS_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1757664355.py\u001b[0m in \u001b[0;36mstore_keypoint_arrays\u001b[0;34m(models, word_dir, out_dir, split, signer, word, max_videos)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mMS_30FPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 video_kps.append(\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mextract_frame_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhands_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 )\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1757664355.py\u001b[0m in \u001b[0;36mextract_frame_keypoints\u001b[0;34m(frame_path, pose_model, face_model, hands_model, timestamp)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnp_xyz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":170},{"cell_type":"code","source":"def load_keypoints(kps_dir, f_avg, split, words=None, signers=None):\n    def pad_seq_(x, padding_amount):\n        x = np.concatenate((x, np.repeat(x[-1], padding_amount, axis=0)), axis=0)\n\n    signers = signers or [\"01\", \"02\", \"03\"]\n    words = words or tuple((f\"{v:04}\" for v in range(1, 503)))\n\n    kps_data_path = os_join(kps_dir, \"all_kps\")\n    sequences = []\n    for word in tqdm(words[:1]):\n        for signer in signers:\n            word_dir = os_join(kps_data_path, f\"{signer}-{split}\", word)\n            sequences.append(\n                [np.load(os_join(word_dir, video)) for video in os.listdir(word_dir)]\n            )\n    return sequences\n    X = np.array(sequences)\n    y = np.array([label_map[word] for word in words])\n    y = OneHotEncoder(sparse=False).fit_transform(y.reshape(-1, 1))\n\n    return X, y\n\n# X, y = load_keypoints(KPS_DIR, SEQ_LEN, \"train\")\nseq = load_keypoints(KPS_DIR, SEQ_LEN, \"train\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(seq), len(seq[0]), len(seq[1]), len(seq[2]), seq[0][0].shape, seq[1][0].shape, seq[2][0].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.concatenate(seq, axis=1).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -cf all-kps /kaggle/working/karsl-kps/all_kps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5612889,"sourceType":"datasetVersion","datasetId":3211537}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\ngdrive_kaggle_wl_ar_sl = user_secrets.get_secret(\"gdrive_kaggle_wl_ar_sl\")\n\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\ndef upload_to_gdrive(file_name, file_content):\n    file_metadata = {\n        'title': file_name,\n        'parents': [{'id': gdrive_kaggle_wl_ar_sl}]\n    }\n    file = drive.CreateFile(file_metadata)\n    file.SetContentString(file_content)\n    file.Upload() # Files.insert()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:00:43.469508Z","iopub.execute_input":"2025-08-03T14:00:43.469825Z","iopub.status.idle":"2025-08-03T14:00:44.472357Z","shell.execute_reply.started":"2025-08-03T14:00:43.469795Z","shell.execute_reply":"2025-08-03T14:00:44.471479Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!wget https://github.com/issamjebnouni/Arabic-Word-level-Sign-Language-Recognition/raw/refs/heads/main/KARSL-502_Labels.xlsx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T22:57:54.525303Z","iopub.execute_input":"2025-08-03T22:57:54.525550Z","iopub.status.idle":"2025-08-03T22:57:54.986481Z","shell.execute_reply.started":"2025-08-03T22:57:54.525530Z","shell.execute_reply":"2025-08-03T22:57:54.985116Z"}},"outputs":[{"name":"stdout","text":"--2025-08-03 22:57:54--  https://github.com/issamjebnouni/Arabic-Word-level-Sign-Language-Recognition/raw/refs/heads/main/KARSL-502_Labels.xlsx\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/issamjebnouni/Arabic-Word-level-Sign-Language-Recognition/refs/heads/main/KARSL-502_Labels.xlsx [following]\n--2025-08-03 22:57:54--  https://raw.githubusercontent.com/issamjebnouni/Arabic-Word-level-Sign-Language-Recognition/refs/heads/main/KARSL-502_Labels.xlsx\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 26778 (26K) [application/octet-stream]\nSaving to: ‘KARSL-502_Labels.xlsx’\n\nKARSL-502_Labels.xl 100%[===================>]  26.15K  --.-KB/s    in 0.002s  \n\n2025-08-03 22:57:54 (13.6 MB/s) - ‘KARSL-502_Labels.xlsx’ saved [26778/26778]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os \nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\n\ndef get_karsl_words_min_frames_cnt():\n    in_dir = \"/kaggle/input/karsl-502\"\n    words = [str(num).zfill(4) for num in range(1,503)]\n    words_frames = defaultdict(lambda: (0, None))\n    for signer in tqdm([1,2,3], desc='signer'):\n        signer = str(signer).zfill(2)\n        signer_dir = os.path.join(in_dir, signer, signer)\n        for split in tqdm(['train', 'test'], desc='split', leave=False):\n            split_dir = os.path.join(signer_dir, split)\n            for word in tqdm(words, desc='words', leave=False):\n                frames = (999, None)\n                word_dir = os.path.join(split_dir, word)\n                for rep in os.listdir(word_dir):\n                    frames_dir = os.path.join(word_dir, rep)\n                    frames_cnt = len(os.listdir(frames_dir))\n                    if frames_cnt < frames[0]:\n                        frames = (frames_cnt, frames_dir)\n                if frames[0] > words_frames[word][0]:\n                    words_frames[word] = frames\n    return words_frames\n\n# words_frames = get_karsl_words_min_frames_cnt()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T11:38:33.767440Z","iopub.execute_input":"2025-08-03T11:38:33.767775Z","iopub.status.idle":"2025-08-03T11:38:33.775782Z","shell.execute_reply.started":"2025-08-03T11:38:33.767750Z","shell.execute_reply":"2025-08-03T11:38:33.774768Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"bad_samples = [\n    # this sample has >260 frames, and after inspection it has many unrelated frames, so just drop it\n    'karsl-502/02/02/train/0443/03_02_0443_(15_11_17_15_52_07)_c',\n]\n\nPAD_TKN = -1\nSEQ_LEN = 80","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T11:14:11.539441Z","iopub.execute_input":"2025-08-03T11:14:11.539793Z","iopub.status.idle":"2025-08-03T11:14:11.545391Z","shell.execute_reply.started":"2025-08-03T11:14:11.539765Z","shell.execute_reply":"2025-08-03T11:14:11.544264Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"# !tar -cf sample.tar.gz '/kaggle/input/karsl-502/03/03/test/0102/03_03_0102_(22_12_16_10_40_19)_c'\n# sorted(words_frames.values())","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mediapipe as mp\nimport cv2\nimport numpy as np\nimport os\nfrom tqdm.notebook import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\n\nmp_holistic = mp.solutions.holistic.Holistic()\n# mp_hands = mp.solutions.hands.Hands()\n# mp_pose = mp.solutions.pose.Pose()\n# mp_face = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n\nmp_face_nose_idx = mp.solutions.face_mesh_connections.FACEMESH_NOSE[0][0]\nmp_hand_wrist_idx = mp.solutions.hands.HandLandmark.WRIST\nmp_pose_nose_idx = mp.solutions.pose.PoseLandmark.NOSE\n\n\npose_kps_idx = tuple(range(11, 17))\nface_kps_idx = tuple(\n    set(\n        point\n        for edge in [\n            mp.solutions.face_mesh_connections.FACEMESH_CONTOURS,\n            mp.solutions.face_mesh_connections.FACEMESH_IRISES,\n        ]\n        for point in edge\n    )\n)\nhand_kps_idx = tuple(\n    set(point for edge in mp.solutions.hands.HAND_CONNECTIONS for point in edge)\n)\n\nPOSE_NUM = len(pose_kps_idx)\nFACE_NUM = len(face_kps_idx)\nHAND_NUM = len(hand_kps_idx)\n# POSE_NUM, FACE_NUM, HAND_NUM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def adjust_keypoints(arr, center):\n#     # arr_reshaped = arr.reshape(-1, 3)\n#     # center_repeated = np.tile(center, (len(arr_reshaped), 1))\n#     # arr_adjusted = arr_reshaped - center_repeated\n#     # return arr_adjusted.reshape(-1)\n#     return arr - center\n\n\ndef extract_frame_keypoints(results):\n    # TODO: normalize(?) keypoints after adjustment\n\n    def get_xyz(lm):\n        return (lm.x, lm.y, lm.z)\n\n    # define numpy views, pose -> face -> rh -> lh\n    all_kps = np.zeros((180, 3))  # (pose=6 + face=132 + rh+lh=42), xyz=3\n    pose_kps = all_kps[:POSE_NUM]\n    face_kps = all_kps[POSE_NUM : POSE_NUM + FACE_NUM]\n    rh_kps = all_kps[POSE_NUM + FACE_NUM : POSE_NUM + FACE_NUM + HAND_NUM]\n    lh_kps = all_kps[POSE_NUM + FACE_NUM + HAND_NUM :]\n\n    def get_pose():\n        nonlocal pose_kps\n        lms = results.pose_landmarks.landmark\n        pose_kps[:] = (get_xyz(lms[idx]) for idx in pose_kps_idx)\n        # pose_kps[:] = adjust_keypoints(pose_kps, pose_kps[0])\n        pose_kps -= pose_kps[mp_pose_nose_idx]\n\n    def get_face():\n        nonlocal face_kps\n        lms = results.face_landmarks.landmark\n        face_kps[:] = (get_xyz(lms[idx]) for idx in face_kps_idx)\n        # face_kps[:] = adjust_keypoints(face_kps, face_kps[0])\n        face_kps -= face_kps[mp_face_nose_idx]\n\n    def get_rh():\n        nonlocal rh_kps\n        rh_kps[:] = (get_xyz(lm) for lm in results.right_hand_landmarks.landmark)\n        # rh_kps[:] = adjust_keypoints(rh_kps, rh_kps[0])\n        rh_kps -= rh_kps[mp_hand_wrist_idx]\n\n    def get_lh():\n        nonlocal lh_kps\n        lh_kps[:] = (get_xyz(lm) for lm in results.left_hand_landmarks.landmark)\n        # lh_kps[:] = adjust_keypoints(lh_kps, lh_kps[0])\n        lh_kps -= lh_kps[mp_hand_wrist_idx]\n\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        executor.submit(get_pose)\n        executor.submit(get_face)\n        executor.submit(get_rh)\n        executor.submit(get_lh)\n\n    return all_kps\n\n\ndef mediapipe_detection(image, model):\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return model.process(image_rgb)\n\n\ndef store_keypoint_arrays(data_dir, out_dir, signer, split, selected_words):\n    \"\"\"This function generates numpy arrays of keypoints for each video in the specified folder location.\n    Args:\n      signer(int): the signer of interest. Could be 01 or 02 or 03\n      split(str): can be 'train', 'test' or 'val'\n    \"\"\"\n    out_dir = os.path.join(out_dir, \"karsl-502\", str(signer), split)\n    os.makedirs(out_dir, exist_ok=True)\n\n    split_dir = os.path.join(data_dir, str(signer), split)\n    for word in tqdm(selected_words):\n        # [pose_dir, face_dir, rh_dir, lh_dir] = [\n        #     os.path.join(out_dir, word, dir_)\n        #     for dir_ in [\"pose_kps\", \"face_kps\", \"rh_kps\", \"lh_kps\"]\n        # ]\n        # for dir_ in [pose_dir, face_dir, rh_dir, lh_dir]:\n        #     os.makedirs(dir_, exist_ok=True)\n\n        word_kps_dir = os.path.join(out_dir, \"all_kps\", f\"{signer}-{split}\", word)\n        os.makedirs(word_kps_dir, exist_ok=True)\n\n        word_dir = os.path.join(split_dir, word)\n        videos = os.listdir(word_dir)\n        for video in videos:\n            video_dir = os.path.join(word_dir, video)\n            video_frames = sorted(os.listdir(video_dir))\n\n            video_kps_dir = os.path.join(word_kps_dir, video)\n            # video_pose_dir = os.path.join(pose_dir, video)\n            # video_face_dir = os.path.join(face_dir, video)\n            # video_rh_dir = os.path.join(rh_dir, video)\n            # video_lh_dir = os.path.join(lh_dir, video)\n\n            # pose_kps, face_kps, lh_kps, rh_kps = [], [], [], []\n            all_kps = []\n            holistic = mp_holistic(\n                min_detection_confidence=0.5, min_tracking_confidence=0.5\n            )\n            with holistic:\n                for frame in video_frames:\n                    frame = cv2.imread(os.path.join(video_dir, frame))\n                    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n                    all_kps.append(\n                        extract_frame_keypoints(mediapipe_detection(frame, holistic))\n                    )\n\n                    # Normalize pixel values to the range [0, 1]\n                    # pose, face, rh, lh = extract_frame_keypoints(\n                    #     mediapipe_detection(frame, holistic)\n                    # )\n\n                    # pose_kps.append(pose)\n                    # face_kps.append(face)\n                    # rh_kps.append(rh)\n                    # lh_kps.append(lh)\n\n                    # np.save(video_pose_dir, pose_kps)\n                    # np.save(video_face_dir, face_kps)\n                    # np.save(video_rh_dir, rh_kps)\n                    # np.save(video_lh_dir, lh_kps)\n\n            np.save(video_kps_dir, all_kps)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_keypoints_from_frames(data_root_dir, signers=None, splits=None):\n    if signers is None:\n        signers = [\"01\", \"02\", \"03\"]\n    if splits is None:\n        splits = [\"train\", \"test\"]\n    for signer in signers:\n        for split in splits:\n            store_keypoint_arrays(data_root_dir, \"./karsl-data/\", signer, split)\n\n\nextract_keypoints_from_frames(\"/kaggle/input/karsl-502\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T23:02:51.226944Z","iopub.execute_input":"2025-08-03T23:02:51.227403Z","iopub.status.idle":"2025-08-03T23:02:51.233588Z","shell.execute_reply.started":"2025-08-03T23:02:51.227370Z","shell.execute_reply":"2025-08-03T23:02:51.232604Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def load_keypoints(kps_data_path, signers, split, words, f_avg):\n    def pad_seq_(x, padding_amount):\n        x = np.concatenate((x, np.repeat(x[-1], padding_amount, axis=0)), axis=0)\n\n    sequences = []\n    for word in tqdm(words):\n        word = f\"{w2id[word]:04}\"\n        for signer in signers:\n            word_dir = os.path.join(kps_data_path, f\"{signer}-{split}\", word)\n            for sequence in os.listdir(os.path.join(word_dir, \"lh_keypoints\")):\n                seq_pose, seq_face, seq_lh, seq_rh = [\n                    np.load(os.path.join(word_dir, kp, sequence))\n                    for kp in [\"pose_kps\", \"face_kps\", \"lh_kps\", \"rh_kps\"]\n                ]\n\n                if f_avg > seq_lh.shape[0]:\n                    padding_amount = f_avg - seq_lh.shape[0]\n                    pad_seq_(seq_pose, padding_amount)\n                    pad_seq_(seq_face, padding_amount)\n                    pad_seq_(seq_lh, padding_amount)\n                    pad_seq_(seq_rh, padding_amount)\n                    # seq_lh = pad_seq(seq_lh, padding_amount)\n                    # seq_rh = pad_seq(seq_rh, padding_amount)\n                    # seq_face = pad_seq(seq_face, padding_amount)\n                    # seq_pose = pad_seq(seq_pose, padding_amount)\n                    # padding = np.repeat(seq_lh[-1], padding_amount, axis=0)\n                    # seq_lh = np.concatenate((seq_lh, padding), axis=0)\n                    # padding = np.repeat(seq_rh[-1], padding_amount, axis=0)\n                    # seq_rh = np.concatenate((seq_rh, padding), axis=0)\n                    # padding = np.repeat(seq_face[-1], padding_amount, axis=0)\n                    # seq_face = np.concatenate((seq_face, padding), axis=0)\n                    # padding = np.repeat(seq_pose[-1], padding_amount, axis=0)\n                    # seq_pose = np.concatenate((seq_pose, padding), axis=0)\n\n                sequences.append(\n                    np.concatenate((seq_pose, seq_face, seq_lh, seq_rh), axis=1)\n                )\n\n    X = np.array(sequences)\n    y = np.array([label_map[word] for word in words])\n    y = OneHotEncoder(sparse=False).fit_transform(y.reshape(-1, 1))\n\n    return X, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"load_keypoints(\"./karsl-data/\", [\"01\", \"02\", \"03\"], \"train\", None, 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:12:11.455713Z","iopub.execute_input":"2025-08-03T14:12:11.456135Z","iopub.status.idle":"2025-08-03T14:12:12.943087Z","shell.execute_reply.started":"2025-08-03T14:12:11.456107Z","shell.execute_reply":"2025-08-03T14:12:12.942224Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
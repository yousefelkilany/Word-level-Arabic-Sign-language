{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5612889,"sourceType":"datasetVersion","datasetId":3211537}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\ngdrive_kaggle_wl_ar_sl = user_secrets.get_secret(\"gdrive_kaggle_wl_ar_sl\")\n\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\ndef upload_to_gdrive(file_name, file_content):\n    file_metadata = {\n        'title': file_name,\n        'parents': [{'id': gdrive_kaggle_wl_ar_sl}]\n    }\n    file = drive.CreateFile(file_metadata)\n    file.SetContentString(file_content)\n    file.Upload() # Files.insert()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T19:30:16.546713Z","iopub.execute_input":"2025-08-04T19:30:16.547126Z","iopub.status.idle":"2025-08-04T19:31:32.946005Z","shell.execute_reply.started":"2025-08-04T19:30:16.547092Z","shell.execute_reply":"2025-08-04T19:31:32.945229Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!export TF_CPP_MIN_LOG_LEVEL=2\n!pip install -q opencv-python mediapipe matplotlib\n!wget -O KARSL-502_Labels.xlsx -q https://github.com/issamjebnouni/Arabic-Word-level-Sign-Language-Recognition/raw/refs/heads/main/KARSL-502_Labels.xlsx\n!wget -O hand_landmarker.task -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\n!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-05T04:05:58.909519Z","iopub.execute_input":"2025-08-05T04:05:58.909909Z","iopub.status.idle":"2025-08-05T04:06:07.722741Z","shell.execute_reply.started":"2025-08-05T04:05:58.909879Z","shell.execute_reply":"2025-08-05T04:06:07.721321Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def upload_to_gdrive(file_name, file_content):\n    file_metadata = {\n        'title': file_name,\n        'parents': [{'id': gdrive_kaggle_wl_ar_sl}]\n    }\n    file = drive.CreateFile(file_metadata)\n    file.SetContentString(file_content)\n    file.Upload() # Files.insert()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport mediapipe as mp\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\nfrom concurrent.futures import ThreadPoolExecutor\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # '2' suppresses warnings and info messages\nos_join = os.path.join\n\nDATA_DIR = \"/kaggle/input/karsl-502\"\nKPS_DIR = \"/kaggle/working/karsl-kps\"\n\nmp_hands = mp.solutions.hands\nmp_pose = mp.solutions.pose\nmp_face = mp.solutions.face_mesh\n\nmp_face_nose_idx = sorted(mp.solutions.face_mesh_connections.FACEMESH_NOSE)[0][0]\nmp_hand_wrist_idx = mp.solutions.hands.HandLandmark.WRIST\nmp_pose_nose_idx = mp.solutions.pose.PoseLandmark.NOSE\n\npose_kps_idx = tuple(\n    (\n        mp.solutions.pose.PoseLandmark.LEFT_SHOULDER,\n        mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER,\n        mp.solutions.pose.PoseLandmark.LEFT_ELBOW,\n        mp.solutions.pose.PoseLandmark.RIGHT_ELBOW,\n        mp.solutions.pose.PoseLandmark.LEFT_WRIST,\n        mp.solutions.pose.PoseLandmark.RIGHT_WRIST,\n    )\n)\nface_kps_idx = tuple(\n    sorted(\n        set(\n            point\n            for edge in [\n                *mp.solutions.face_mesh_connections.FACEMESH_CONTOURS,\n                *mp.solutions.face_mesh_connections.FACEMESH_IRISES,\n            ]\n            for point in edge\n        )\n    )\n)\nhand_kps_idx = tuple(range(len(mp.solutions.hands.HandLandmark)))\n\nPOSE_NUM = len(pose_kps_idx)\nFACE_NUM = len(face_kps_idx)\nHAND_NUM = len(hand_kps_idx)\n\nKP2SLICE = {\n    \"pose\": slice(0, POSE_NUM),\n    \"face\": slice(POSE_NUM, POSE_NUM + FACE_NUM),\n    \"rh\": slice(POSE_NUM + FACE_NUM, POSE_NUM + FACE_NUM + HAND_NUM),\n    \"lh\": slice(POSE_NUM + FACE_NUM + HAND_NUM, POSE_NUM + FACE_NUM + HAND_NUM * 2),\n}\nPOSE_KPS2IDX = {kps: idx for idx, kps in enumerate(pose_kps_idx)}\nFACE_KPS2IDX = {kps: idx for idx, kps in enumerate(face_kps_idx)}\nHAND_KPS2IDX = {kps: idx for idx, kps in enumerate(hand_kps_idx)}\nKPS2IDX = {\"pose\": POSE_KPS2IDX, \"face\": FACE_KPS2IDX, \"hand\": HAND_KPS2IDX}\n\n\n# usage: use it to draw mediapipe connections with the kps loaded from `.npy`arrays\nfor u, v in list(mp.solutions.face_mesh_connections.FACEMESH_IRISES)[:3]:\n    print(face_kps_idx[FACE_KPS2IDX[u]], face_kps_idx[FACE_KPS2IDX[v]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T03:45:37.599569Z","iopub.execute_input":"2025-08-05T03:45:37.599992Z","iopub.status.idle":"2025-08-05T03:45:56.521200Z","shell.execute_reply.started":"2025-08-05T03:45:37.599950Z","shell.execute_reply":"2025-08-05T03:45:56.519527Z"}},"outputs":[{"name":"stderr","text":"2025-08-05 03:45:39.937642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754365540.204935      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754365540.281032      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"475 476\n477 474\n469 470\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def get_karsl_words_min_frames_cnt():\n    in_dir = \"/kaggle/input/karsl-502\"\n    words_frames = defaultdict(lambda: (0, None))\n    for signer in tqdm([\"01\", \"02\", \"03\"], desc=\"signer\"):\n        signer_dir = os_join(in_dir, signer, signer)\n\n        for split in tqdm([\"train\", \"test\"], desc=\"split\", leave=False):\n            split_dir = os_join(signer_dir, split)\n\n            for word in tqdm(range(1, 503), desc=\"words\", leave=False):\n                frames = (999, None)\n                word_dir = os_join(split_dir, f\"{word:04}\")\n\n                for rep in os.listdir(word_dir):\n                    frames_dir = os_join(word_dir, rep)\n                    frames_cnt = len(os.listdir(frames_dir))\n                    if frames_cnt < frames[0]:\n                        frames = (frames_cnt, frames_dir)\n\n                if frames[0] > words_frames[word][0]:\n                    words_frames[word] = frames\n    return words_frames\n\n\n# words_frames = get_karsl_words_min_frames_cnt()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !tar -cf sample.tar.gz '/kaggle/input/karsl-502/03/03/test/0102/03_03_0102_(22_12_16_10_40_19)_c'\n# sorted(words_frames.values())","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bad_samples = [\n    # this sample has >260 frames, and after inspection it has many unrelated frames, so just drop it\n    'karsl-502/02/02/train/0443/03_02_0443_(15_11_17_15_52_07)_c',\n]\n\nPAD_TKN = -1\nSEQ_LEN = 80","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T04:02:39.233939Z","iopub.execute_input":"2025-08-05T04:02:39.234262Z","iopub.status.idle":"2025-08-05T04:02:42.688985Z","shell.execute_reply.started":"2025-08-05T04:02:39.234240Z","shell.execute_reply":"2025-08-05T04:02:42.687652Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"from mediapipe import solutions\nfrom mediapipe.framework.formats import landmark_pb2\nimport numpy as np\n\nMARGIN = 10  # pixels\nFONT_SIZE = 1\nFONT_THICKNESS = 1\nHANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n\ndef draw_landmarks_on_image(rgb_image, detection_result):\n  hand_landmarks_list = detection_result.hand_landmarks\n  handedness_list = detection_result.handedness\n  annotated_image = np.copy(rgb_image)\n\n  # Loop through the detected hands to visualize.\n  for idx in range(len(hand_landmarks_list)):\n    hand_landmarks = hand_landmarks_list[idx]\n    handedness = handedness_list[idx]\n\n    # Draw the hand landmarks.\n    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n    hand_landmarks_proto.landmark.extend([\n      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n    ])\n    solutions.drawing_utils.draw_landmarks(\n      annotated_image,\n      hand_landmarks_proto,\n      solutions.hands.HAND_CONNECTIONS,\n      solutions.drawing_styles.get_default_hand_landmarks_style(),\n      solutions.drawing_styles.get_default_hand_connections_style())\n\n    # Get the top left corner of the detected hand's bounding box.\n    height, width, _ = annotated_image.shape\n    x_coordinates = [landmark.x for landmark in hand_landmarks]\n    y_coordinates = [landmark.y for landmark in hand_landmarks]\n    text_x = int(min(x_coordinates) * width)\n    text_y = int(min(y_coordinates) * height) - MARGIN\n\n    # Draw handedness (left or right hand) on the image.\n    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n\n  return annotated_image\n\ndef draw_pose_andmarks_on_image(rgb_image, detection_result):\n  pose_landmarks_list = detection_result.pose_landmarks\n  annotated_image = np.copy(rgb_image)\n\n  # Loop through the detected poses to visualize.\n  for idx in range(len(pose_landmarks_list)):\n    pose_landmarks = pose_landmarks_list[idx]\n    print(pose_landmarks)\n    # Draw the pose landmarks.\n    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n    # lms_list = [landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks]\n    lms_list = [landmark_pb2.NormalizedLandmark(x=0, y=0, z=0) for landmark in pose_landmarks]\n    for idx in pose_kps_idx:\n        lms_list[idx] = landmark_pb2.NormalizedLandmark(x=pose_landmarks[idx.value].x, y=pose_landmarks[idx.value].y, z=pose_landmarks[idx.value].z) \n    pose_landmarks_proto.landmark.extend(lms_list)\n    \n    print(sorted(solutions.pose.POSE_CONNECTIONS))\n    solutions.drawing_utils.draw_landmarks(\n      annotated_image,\n      pose_landmarks_proto,\n      # solutions.pose.POSE_CONNECTIONS,\n      [(11, 12), (11, 13), (13, 15), (12, 14), (14, 16)],\n      solutions.drawing_styles.get_default_pose_landmarks_style()\n    )\n  return annotated_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T04:32:27.878149Z","iopub.execute_input":"2025-08-05T04:32:27.878488Z","iopub.status.idle":"2025-08-05T04:32:27.891542Z","shell.execute_reply.started":"2025-08-05T04:32:27.878465Z","shell.execute_reply":"2025-08-05T04:32:27.890424Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"image_dir = '/kaggle/input/karsl-502/01/01/train/0001/01_01_0001_(10_11_16_16_21_34)_c/'\nimage_path = os.path.join(image_dir, os.listdir(image_dir)[8])\nshutil.copy(image_path, '/kaggle/working')\n# frame = cv2.imread(image_path)\nframe = mp.Image.create_from_file(image_path)\n\n\nimport mediapipe as mp\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\n\n# base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n# options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2)\n# hands_model = vision.HandLandmarker.create_from_options(options)\n# hands_res = hands_model.detect(frame)\n# print(dir(hands_res))\n# print(hands_res.handedness[0][0].category_name)\n# print(len(hands_res.hand_landmarks[0]))\n\n# base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n# options = vision.FaceLandmarkerOptions(base_options=base_options, num_faces=1)\n# face_model = vision.FaceLandmarker.create_from_options(options)\n# face_res = face_model.detect(frame)\n# print(dir(face_res))\n# print(face_res.facial_transformation_matrixes)\n# print(len(face_res.face_landmarks[0]))\n\nbase_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\noptions = vision.PoseLandmarkerOptions(base_options=base_options)\npose_model = vision.PoseLandmarker.create_from_options(options)\npose_res = pose_model.detect(frame)\nprint(dir(pose_res))\nprint(len(pose_res.pose_landmarks[0]))\n\n# annotated_image = draw_landmarks_on_image(frame.numpy_view(), hands_res)\nannotated_image = draw_pose_andmarks_on_image(frame.numpy_view(), pose_res)\ncv2.imwrite(\"drawn.jpg\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n\n\n# hands_model = mp_hands.Hands()\n# hands_res = hands_model.process(frame)\n# print(hands_res.multi_handedness, hands_res.multi_hand_landmarks)\n# print(len(pose_res.pose_landmarks.landmark), len(face_res.multi_face_landmarks[0].landmark))\n# pose_model = mp_pose.Pose()\n# pose_res = pose_model.process(frame)\n# face_model = mp_face.FaceMesh(refine_landmarks=True)\n# face_res = face_model.process(frame)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T04:32:31.449586Z","iopub.execute_input":"2025-08-05T04:32:31.449952Z","iopub.status.idle":"2025-08-05T04:32:31.801609Z","shell.execute_reply.started":"2025-08-05T04:32:31.449927Z","shell.execute_reply":"2025-08-05T04:32:31.800217Z"}},"outputs":[{"name":"stdout","text":"['__annotations__', '__class__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'pose_landmarks', 'pose_world_landmarks', 'segmentation_masks']\n33\n[NormalizedLandmark(x=0.4745466411113739, y=0.3982640504837036, z=-0.6154065728187561, visibility=0.9994738698005676, presence=0.9988082647323608), NormalizedLandmark(x=0.48750513792037964, y=0.35941606760025024, z=-0.5732194781303406, visibility=0.9986727237701416, presence=0.9977400302886963), NormalizedLandmark(x=0.5000902414321899, y=0.3578551411628723, z=-0.5736538767814636, visibility=0.9984139204025269, presence=0.9977278113365173), NormalizedLandmark(x=0.51067054271698, y=0.3570733666419983, z=-0.5737800002098083, visibility=0.9987255930900574, presence=0.9966908693313599), NormalizedLandmark(x=0.4495016038417816, y=0.3652740716934204, z=-0.5704296231269836, visibility=0.9991748929023743, presence=0.9970833659172058), NormalizedLandmark(x=0.43864011764526367, y=0.36718112230300903, z=-0.5714367032051086, visibility=0.9991647005081177, presence=0.9973970651626587), NormalizedLandmark(x=0.4261924624443054, y=0.3709453344345093, z=-0.5720376372337341, visibility=0.9993215799331665, presence=0.9967544674873352), NormalizedLandmark(x=0.5279719233512878, y=0.36812645196914673, z=-0.28443220257759094, visibility=0.9990940093994141, presence=0.9967564940452576), NormalizedLandmark(x=0.40928149223327637, y=0.3873596787452698, z=-0.27765461802482605, visibility=0.99871826171875, presence=0.9965819716453552), NormalizedLandmark(x=0.5020069479942322, y=0.42639195919036865, z=-0.4985105097293854, visibility=0.9994749426841736, presence=0.998748779296875), NormalizedLandmark(x=0.4541124701499939, y=0.4353322982788086, z=-0.5022686719894409, visibility=0.9995490908622742, presence=0.9982389211654663), NormalizedLandmark(x=0.6311517357826233, y=0.5346550941467285, z=-0.12954890727996826, visibility=0.9997019171714783, presence=0.9993098974227905), NormalizedLandmark(x=0.34434229135513306, y=0.5659582614898682, z=-0.04697680473327637, visibility=0.9996387958526611, presence=0.9992340803146362), NormalizedLandmark(x=0.6996482610702515, y=0.807420551776886, z=-0.4023876190185547, visibility=0.9929959177970886, presence=0.9906002283096313), NormalizedLandmark(x=0.2806078791618347, y=0.8113471865653992, z=-0.03669058904051781, visibility=0.7862429022789001, presence=0.9916360378265381), NormalizedLandmark(x=0.6461342573165894, y=0.5586619973182678, z=-0.9118019342422485, visibility=0.9960756897926331, presence=0.9999462366104126), NormalizedLandmark(x=0.38181936740875244, y=0.9809535145759583, z=-0.268342524766922, visibility=0.5640199780464172, presence=0.9801380038261414), NormalizedLandmark(x=0.6477180123329163, y=0.49275368452072144, z=-1.0136473178863525, visibility=0.9918979406356812, presence=0.9998949766159058), NormalizedLandmark(x=0.3919503688812256, y=1.0366555452346802, z=-0.3264601528644562, visibility=0.43709897994995117, presence=0.927828311920166), NormalizedLandmark(x=0.6329959630966187, y=0.47285860776901245, z=-1.0119447708129883, visibility=0.9918386340141296, presence=0.9999308586120605), NormalizedLandmark(x=0.42378222942352295, y=1.0196934938430786, z=-0.3597038984298706, visibility=0.4670525789260864, presence=0.949056088924408), NormalizedLandmark(x=0.622040331363678, y=0.49522262811660767, z=-0.9169926047325134, visibility=0.9871829748153687, presence=0.9999586343765259), NormalizedLandmark(x=0.428221195936203, y=0.9936448335647583, z=-0.28686004877090454, visibility=0.5210910439491272, presence=0.9718547463417053), NormalizedLandmark(x=0.6227591037750244, y=0.9737582206726074, z=0.006482444703578949, visibility=0.9702388048171997, presence=0.8998953104019165), NormalizedLandmark(x=0.4323602318763733, y=0.9803932309150696, z=-0.007011017296463251, visibility=0.9751003384590149, presence=0.8975505828857422), NormalizedLandmark(x=0.6642961502075195, y=1.3071446418762207, z=-0.020632833242416382, visibility=0.03101992979645729, presence=0.0250664371997118), NormalizedLandmark(x=0.45117634534835815, y=1.280258059501648, z=-0.2717079818248749, visibility=0.056786295026540756, presence=0.025571035221219063), NormalizedLandmark(x=0.6578623652458191, y=1.5712838172912598, z=0.2601519227027893, visibility=0.005786612629890442, presence=0.003854642156511545), NormalizedLandmark(x=0.4629552960395813, y=1.5876359939575195, z=0.055319707840681076, visibility=0.011917362920939922, presence=0.0020743890199810266), NormalizedLandmark(x=0.662489652633667, y=1.6056808233261108, z=0.27544644474983215, visibility=0.00371789513155818, presence=0.002171915490180254), NormalizedLandmark(x=0.46490737795829773, y=1.6300804615020752, z=0.07862192392349243, visibility=0.008907441981136799, presence=0.002509801182895899), NormalizedLandmark(x=0.6173275709152222, y=1.6752076148986816, z=0.03158515319228172, visibility=0.004182956647127867, presence=0.003462533000856638), NormalizedLandmark(x=0.48883944749832153, y=1.6928987503051758, z=-0.15591581165790558, visibility=0.006623243913054466, presence=0.0019216613145545125)]\n[(0, 1), (0, 4), (1, 2), (2, 3), (3, 7), (4, 5), (5, 6), (6, 8), (9, 10), (11, 12), (11, 13), (11, 23), (12, 14), (12, 24), (13, 15), (14, 16), (15, 17), (15, 19), (15, 21), (16, 18), (16, 20), (16, 22), (17, 19), (18, 20), (23, 24), (23, 25), (24, 26), (25, 27), (26, 28), (27, 29), (27, 31), (28, 30), (28, 32), (29, 31), (30, 32)]\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754368351.458441      36 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL\nW0000 00:00:1754368351.527697     741 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1754368351.667296     743 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"},{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":130},{"cell_type":"code","source":"def extract_frame_keypoints(frame, pose_model, face_model, hands_model):\n    # TODO: normalize(?) keypoints after adjustment\n\n    def get_xyz(lm):\n        return (lm.x, lm.y, lm.z)\n\n    # define numpy views, pose -> face -> rh -> lh\n    all_kps = np.zeros((184, 3))  # (pose=6 + face=136 + rh+lh=42), xyz=3\n    pose_kps = all_kps[KP2SLICE[\"pose\"]]\n    face_kps = all_kps[KP2SLICE[\"face\"]]\n    rh_kps = all_kps[KP2SLICE[\"rh\"]]\n    lh_kps = all_kps[KP2SLICE[\"lh\"]]\n    np_xyz = np.dtype((float, 3))\n\n    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    \n    def get_pose():\n        nonlocal pose_kps\n        results = pose_model.process(image_rgb)\n        if results.pose_landmarks is None:\n            return\n\n        lms = results.pose_landmarks.landmark\n        pose_kps[:] = np.fromiter(((lms[idx].x, lms[idx].y, lms[idx].z) for idx in pose_kps_idx), dtype=np_xyz)\n        # pose_kps -= pose_kps[mp_pose_nose_idx]\n\n    def get_face():\n        nonlocal face_kps\n        results = face_model.process(image_rgb)\n        if results.multi_face_landmarks is None:\n            return\n\n        lms = results.multi_face_landmarks[0].landmark\n        face_kps[:] = np.fromiter(((lms[idx].x, lms[idx].y, lms[idx].z) for idx in face_kps_idx), dtype=np_xyz)\n        # face_kps -= face_kps[mp_face_nose_idx]\n\n    def get_hands():\n        nonlocal rh_kps, lh_kps\n        results = hands_model.process(image_rgb)\n        if results.multi_hand_landmarks is None:\n            return\n\n        for handedness, hand_landmarks in zip(results.multi_handedness, results.multi_hand_landmarks):\n            hand_type = handedness.classification[0].index\n            target_kps = rh_kps if hand_type == 1 else lh_kps # Assuming 1 for Right, 0 for Left\n            target_kps[:] = np.array([(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark], dtype=np.float32)\n        \n        for handedness, hand_landmarks in zip(results.multi_handedness, results.multi_hand_landmarks):\n            lms = hand_landmarks.landmark\n            target_hand = rh_kps if handedness.classification[0].index == 1 else rh_kps\n            target_hand[:] = np.fromiter(((lm.x, lm.y, lm.z) for lm in lms), dtype=np_xyz)\n               \n            if results.multi_handedness[i].classification[0].index == 0:\n                lms = hand_landmarks.landmark\n                target_hand = \n                rh_kps[:] = np.fromiter(((lm.x, lm.y, lm.z) for lm in lms), dtype=np_xyz)\n                # rh_kps -= rh_kps[mp_hand_wrist_idx]\n            else:\n                lms = hand_landmarks.landmark\n                lh_kps[:] = np.fromiter(((lm.x, lm.y, lm.z) for lm in lms), dtype=np_xyz)\n                # lh_kps -= lh_kps[mp_hand_wrist_idx]\n\n    with ThreadPoolExecutor(max_workers=3) as executor:\n        executor.submit(get_pose)\n        executor.submit(get_face)\n        executor.submit(get_hands)\n    \n    return all_kps\n\n\ndef store_keypoint_arrays(models, word_dir, out_dir, split, signer, word):\n    [pose_model, face_model, hands_model] = models\n\n    all_kps = []\n    videos_bar = tqdm(os.listdir(word_dir), leave=False)\n    for video in videos_bar:\n        video_dir = os_join(word_dir, video)\n        videos_bar.set_description(f\"Current video: {video}\")\n\n        video_kps = []\n        for frame in sorted(os.listdir(video_dir)):\n            frame = cv2.imread(os_join(video_dir, frame))\n            video_kps.append(\n                extract_frame_keypoints(frame, pose_model, face_model, hands_model)\n            )\n\n        pose_model.reset()\n        face_model.reset()\n        hands_model.reset()\n\n        all_kps.append(video_kps.copy())\n\n    word_kps_path = os_join(out_dir, \"all_kps\", f\"{signer}-{split}\", word)\n    os.makedirs(os.dirname(word_kps_path), exist_ok=True)\n    np.savez(word_kps_path, keypoints=np.concatenate(all_kps, axis=0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_keypoints_from_frames(data_dir, kps_dir, splits=None, signers=None, selected_words=None):\n    pose_model = mp_pose.Pose()\n    face_model = mp_face.FaceMesh(refine_landmarks=True)\n    hands_model = mp_hands.Hands()\n    models = [pose_model, face_model, hands_model]\n\n    splits = splits or [\"train\", \"test\"]\n    signers = signers or [\"01\", \"02\", \"03\"]\n    selected_words = selected_words or tuple((f\"{v:04}\" for v in range(46, 503)))\n    words_bar = tqdm(selected_words)\n    for word in words_bar:\n        words_bar.set_description(f\"Current word: {word}\")\n        signers_bar = tqdm(signers, leave=False)\n        for signer in signers:\n            signers_bar.set_description(f\"Current signer: {signer}\")\n            splits_bar = tqdm(splits, leave=False)\n            for split in splits:\n                splits_bar.set_description(f\"Current split: {split}\")\n                word_dir = os_join(data_dir, signer, signer, split, word)\n                store_keypoint_arrays(models, word_dir, kps_dir, split, signer, word, max_videos=10)\n    \nextract_keypoints_from_frames(DATA_DIR, KPS_DIR)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_keypoints(kps_dir, f_avg, split, words=None, signers=None):\n    def pad_seq_(x, padding_amount):\n        x = np.concatenate((x, np.repeat(x[-1], padding_amount, axis=0)), axis=0)\n\n    signers = signers or [\"01\", \"02\", \"03\"]\n    words = words or tuple((f\"{v:04}\" for v in range(1, 503)))\n\n    kps_data_path = os_join(kps_dir, \"all_kps\")\n    sequences = []\n    for word in tqdm(words[:1]):\n        for signer in signers:\n            word_dir = os_join(kps_data_path, f\"{signer}-{split}\", word)\n            sequences.append(\n                [np.load(os_join(word_dir, video)) for video in os.listdir(word_dir)]\n            )\n    return sequences\n    X = np.array(sequences)\n    y = np.array([label_map[word] for word in words])\n    y = OneHotEncoder(sparse=False).fit_transform(y.reshape(-1, 1))\n\n    return X, y\n\n# X, y = load_keypoints(KPS_DIR, SEQ_LEN, \"train\")\nseq = load_keypoints(KPS_DIR, SEQ_LEN, \"train\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(seq), len(seq[0]), len(seq[1]), len(seq[2]), seq[0][0].shape, seq[1][0].shape, seq[2][0].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.concatenate(seq, axis=1).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -cf all-kps /kaggle/working/karsl-kps/all_kps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}